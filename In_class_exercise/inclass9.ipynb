{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inclass9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOCXFY4YGcg8EvMwBu69mZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likhitha-kadiyala/Sai-Likhitha_INFO5731_Spring2021/blob/main/In_class_exercise/inclass9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpkkrUFyWlbX"
      },
      "source": [
        "# **The ninth in-class-exercise (20 points in total, 4/16/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAISp22OWlbc"
      },
      "source": [
        "The purpose of the exercise is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from here: https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/exercise09_datacollection.zip. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QjBcZBsIIW5"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, KFold, cross_val_score\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "i3-oLME_JlT8",
        "outputId": "46c1525c-377b-4d75-f8f2-ccf841ee3cd5"
      },
      "source": [
        "from google.colab import files\n",
        "loaded_files=files.upload ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-afe4606a-e397-4e5c-8d52-36ccb83977cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-afe4606a-e397-4e5c-8d52-36ccb83977cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving stsa-test.txt to stsa-test.txt\n",
            "Saving stsa-train.txt to stsa-train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvbKUUGiNKL1",
        "outputId": "ba5a6657-b3f7-434a-d6d2-6d665b4efc33"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aE7uAdDK8Nj"
      },
      "source": [
        "train_dataframe=pd.read_fwf(\"stsa-train.txt\", header=None)\n",
        "#train_dataframe= pd.DataFrame(my_data_train)\n",
        "train_dataframe = train_dataframe.rename(columns={0: \"Sentiment\", 1: \"Text\"})\n",
        "test_dataframe = pd.read_fwf(\"stsa-test.txt\", header=None)\n",
        "#test_dataframe =  pd.DataFrame(my_data_test)\n",
        "test_dataframe = test_dataframe.rename(columns={0: \"Sentiment\", 1: \"Text\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRMr5ZXJMe_r",
        "outputId": "4a5a2976-33c8-43ff-916b-7d40e5571361"
      },
      "source": [
        "#Removal of special characters:\n",
        "\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Text'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "\n",
        "#Removal of punctuations :\n",
        "\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Cleaned_Data'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Cleaned_Data'].apply(lambda x :x.lower())\n",
        "\n",
        "#Removing stopwords by using stopwordslist\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Cleaned_Data'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Cleaned_Data'].apply(lambda x: TextBlob(x).words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8oCV7ZLN44g"
      },
      "source": [
        "from textblob import Word\n",
        "\n",
        "train_dataframe['Cleaned_Data'] = train_dataframe['Cleaned_Data'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0lbBOA8OGJ8"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "train_tfidf_vect = TfidfVectorizer(analyzer = 'word')\n",
        "X_data = train_tfidf_vect.fit_transform(train_dataframe['Cleaned_Data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKeQ90v-OQRQ"
      },
      "source": [
        "x_train_data, x_validation_data, y_train_data, y_validation_data = train_test_split(X_data, train_dataframe['Sentiment'],test_size=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmA4Q1pOOggg",
        "outputId": "f40bfd39-f17e-46dd-f786-2d09bc65c21e"
      },
      "source": [
        "test_dataframe['Cleaned_Data'] = test_dataframe['Text'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "\n",
        "#Removal of punctuations :\n",
        "\n",
        "test_dataframe['Cleaned_Data'] = test_dataframe['Cleaned_Data'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "test_dataframe['Cleaned_Data'] = test_dataframe['Cleaned_Data'].apply(lambda x : x.lower())\n",
        "\n",
        "#Removing stopwords by using stopwordslist\n",
        "stop = stopwords.words('english')\n",
        "test_dataframe['Cleaned_Data'] = test_dataframe['Cleaned_Data'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "test_dataframe['Cleaned_Data'] = test_dataframe['Cleaned_Data'].apply(lambda x: TextBlob(x).words)\n",
        "\n",
        "from textblob import Word\n",
        "test_dataframe['Cleaned_Data'] = test_dataframe['Cleaned_Data'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j3e-Pb0PCgv"
      },
      "source": [
        "test_tfidf_vect1 = TfidfVectorizer(analyzer = 'word', vocabulary = train_tfidf_vect.vocabulary_)\n",
        "test_tfidf_vect1.fit(test_dataframe['Cleaned_Data'])\n",
        "x_test_data = test_tfidf_vect1.transform(test_dataframe['Cleaned_Data'])\n",
        "y_test_data = test_dataframe['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsd_AFvgPP_f",
        "outputId": "6dcaa208-3247-4d29-85cb-786571cc86e1"
      },
      "source": [
        "\n",
        "#Multinomial-Model\n",
        "multinomial_model = MultinomialNB()\n",
        "multinomial_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_MNB = multinomial_model.predict(x_validation_data)\n",
        "validation_accuracy_MNB = round(accuracy_score(validation_predicted_MNB,y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_MNB)\n",
        "print(classification_report(y_validation_data, validation_predicted_MNB))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(multinomial_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_MNB = multinomial_model.predict(x_test_data)\n",
        "test_accuracy_MNB = round(accuracy_score(test_predicted_MNB, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_MNB)\n",
        "print(classification_report(y_test_data, test_predicted_MNB))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  77.82\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.68      0.75       661\n",
            "           1       0.75      0.86      0.80       723\n",
            "\n",
            "    accuracy                           0.78      1384\n",
            "   macro avg       0.79      0.77      0.77      1384\n",
            "weighted avg       0.78      0.78      0.78      1384\n",
            "\n",
            "Corss Validation Score -  72.105\n",
            "Testing Data\n",
            "Accuracy -  79.24\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.69      0.77       912\n",
            "           1       0.74      0.89      0.81       909\n",
            "\n",
            "    accuracy                           0.79      1821\n",
            "   macro avg       0.80      0.79      0.79      1821\n",
            "weighted avg       0.80      0.79      0.79      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8z4UZaRSS8N",
        "outputId": "1b5028b8-eed1-4712-ac34-233bdb22d38f"
      },
      "source": [
        "#svm-Model\n",
        "\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_svm = svm_model.predict(x_validation_data)\n",
        "validation_accuracy_svm = round(accuracy_score(validation_predicted_svm,y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_svm)\n",
        "print(classification_report(y_validation_data, validation_predicted_svm))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(svm_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_svm = svm_model.predict(x_test_data)\n",
        "test_accuracy_svm = round(accuracy_score(test_predicted_svm, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_svm)\n",
        "print(classification_report(y_test_data, test_predicted_svm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  76.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.73      0.75       661\n",
            "           1       0.77      0.80      0.78       723\n",
            "\n",
            "    accuracy                           0.77      1384\n",
            "   macro avg       0.77      0.77      0.77      1384\n",
            "weighted avg       0.77      0.77      0.77      1384\n",
            "\n",
            "Corss Validation Score -  70.568\n",
            "Testing Data\n",
            "Accuracy -  78.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       912\n",
            "           1       0.77      0.82      0.79       909\n",
            "\n",
            "    accuracy                           0.79      1821\n",
            "   macro avg       0.79      0.79      0.79      1821\n",
            "weighted avg       0.79      0.79      0.79      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63DCsMhIQOR-",
        "outputId": "01500053-0e17-4bc7-bfa3-bce689f5a2dd"
      },
      "source": [
        "#KNN-Model\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors = 10)\n",
        "knn_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_KNN = knn_model.predict(x_validation_data)\n",
        "validation_accuracy_KNN = round(accuracy_score(validation_predicted_KNN, y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_KNN)\n",
        "print(classification_report(y_validation_data, validation_predicted_KNN))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(knn_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_KNN = knn_model.predict(x_test_data)\n",
        "test_accuracy_KNN = round(accuracy_score(test_predicted_KNN, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_KNN)\n",
        "print(classification_report(y_test_data, test_predicted_KNN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  66.18\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.78      0.69       661\n",
            "           1       0.73      0.55      0.63       723\n",
            "\n",
            "    accuracy                           0.66      1384\n",
            "   macro avg       0.67      0.67      0.66      1384\n",
            "weighted avg       0.68      0.66      0.66      1384\n",
            "\n",
            "Corss Validation Score -  54.528\n",
            "Testing Data\n",
            "Accuracy -  68.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.80      0.72       912\n",
            "           1       0.74      0.57      0.65       909\n",
            "\n",
            "    accuracy                           0.69      1821\n",
            "   macro avg       0.70      0.69      0.68      1821\n",
            "weighted avg       0.70      0.69      0.68      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn7NeFbyQo_a",
        "outputId": "aee81c09-4f7c-4ef7-d53d-55c85ff1ba30"
      },
      "source": [
        "\n",
        "#DecisionTree\n",
        "\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_DT = dt_model.predict(x_validation_data)\n",
        "validation_accuracy_DT = round(accuracy_score(validation_predicted_DT, y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_DT)\n",
        "print(classification_report(y_validation_data, validation_predicted_DT))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(dt_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_DT = dt_model.predict(x_test_data)\n",
        "test_accuracy_DT = round(accuracy_score(test_predicted_DT, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_DT)\n",
        "print(classification_report(y_test_data, test_predicted_DT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  65.68\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.66      0.65       661\n",
            "           1       0.68      0.66      0.67       723\n",
            "\n",
            "    accuracy                           0.66      1384\n",
            "   macro avg       0.66      0.66      0.66      1384\n",
            "weighted avg       0.66      0.66      0.66      1384\n",
            "\n",
            "Corss Validation Score -  62.768\n",
            "Testing Data\n",
            "Accuracy -  66.45\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.67      0.67       912\n",
            "           1       0.66      0.66      0.66       909\n",
            "\n",
            "    accuracy                           0.66      1821\n",
            "   macro avg       0.66      0.66      0.66      1821\n",
            "weighted avg       0.66      0.66      0.66      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvIeigglQ7Pw",
        "outputId": "f1cba1a7-98a8-4789-de3c-04dd186eaa28"
      },
      "source": [
        "#RandomForest- Classifier\n",
        "\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_RF = rf_model.predict(x_validation_data)\n",
        "validation_accuracy_RF = round(accuracy_score(validation_predicted_RF, y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_RF)\n",
        "print(classification_report(y_validation_data, validation_predicted_RF))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(rf_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_RF = rf_model.predict(x_test_data)\n",
        "test_accuracy_RF = round(accuracy_score(test_predicted_RF, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_RF)\n",
        "print(classification_report(y_test_data, test_predicted_RF))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  72.18\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.67      0.70       661\n",
            "           1       0.72      0.77      0.74       723\n",
            "\n",
            "    accuracy                           0.72      1384\n",
            "   macro avg       0.72      0.72      0.72      1384\n",
            "weighted avg       0.72      0.72      0.72      1384\n",
            "\n",
            "Corss Validation Score -  66.062\n",
            "Testing Data\n",
            "Accuracy -  74.35000000000001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.72      0.74       912\n",
            "           1       0.73      0.76      0.75       909\n",
            "\n",
            "    accuracy                           0.74      1821\n",
            "   macro avg       0.74      0.74      0.74      1821\n",
            "weighted avg       0.74      0.74      0.74      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u7dPaWuRVCC",
        "outputId": "d6f60fb3-3529-4c97-adc0-03e07bc008c0"
      },
      "source": [
        "#XGB-Classifier\n",
        "\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(x_train_data, y_train_data)\n",
        "validation_predicted_xgb = xgb_model.predict(x_validation_data)\n",
        "validation_accuracy_xgb = round(accuracy_score(validation_predicted_xgb, y_validation_data),4)*100\n",
        "print(\"Validation Data\")\n",
        "print(\"Accuracy - \", validation_accuracy_xgb)\n",
        "print(classification_report(y_validation_data, validation_predicted_xgb))\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "print(\"Corss Validation Score - \", round(cross_val_score(xgb_model, x_test_data, y_test_data, cv=kfold).mean()*100,3))\n",
        "test_predicted_xgb = xgb_model.predict(x_test_data)\n",
        "test_accuracy_xgb = round(accuracy_score(test_predicted_xgb, y_test_data),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_xgb)\n",
        "print(classification_report(y_test_data, test_predicted_xgb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "Accuracy -  63.800000000000004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.41      0.52       661\n",
            "           1       0.61      0.85      0.71       723\n",
            "\n",
            "    accuracy                           0.64      1384\n",
            "   macro avg       0.66      0.63      0.61      1384\n",
            "weighted avg       0.66      0.64      0.62      1384\n",
            "\n",
            "Corss Validation Score -  62.052\n",
            "Testing Data\n",
            "Accuracy -  64.25\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.55       912\n",
            "           1       0.60      0.85      0.70       909\n",
            "\n",
            "    accuracy                           0.64      1821\n",
            "   macro avg       0.67      0.64      0.63      1821\n",
            "weighted avg       0.67      0.64      0.63      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzHH2O8NRf5b"
      },
      "source": [
        "Accuracies:\n",
        "\n",
        "MultinomialNB - 72.10%\n",
        "SVM - 78.8%\n",
        "KNN - 68.7%\n",
        "Decision Tree - 66.45%\n",
        "Random Forest - 74.35%\n",
        "XG Boost - 64.25%"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}