{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpBRGeMRFrxKlMc8utTfuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likhitha-kadiyala/Sai-Likhitha_INFO5731_Spring2021/blob/main/In_class_exercise/In_class_exercise4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umHUkEANNFVT"
      },
      "source": [
        "#1.1\r\n",
        "import requests as re\r\n",
        "link = \"https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt\"\r\n",
        "data = re.get(link) \r\n",
        "text_data = data.text \r\n",
        "#print(text2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m17OfYsQNLqw",
        "outputId": "a32ac9e9-5243-46ae-8218-0ab0ebf82af1"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "#Number of sentences\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "print(\"NUmber of sentences\",len(sent_tokenize(text_data)))\r\n",
        "\r\n",
        "#Number of words\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "print(\"number of words\",len(word_tokenize(text_data)))\r\n",
        "\r\n",
        "#Number of characters\r\n",
        "print(\"Number of characters\",len(text_data))\r\n",
        "\r\n",
        "#Average word length\r\n",
        "tokenize_words = word_tokenize(text_data)  \r\n",
        "words_length = []\r\n",
        "for word in tokenize_words:\r\n",
        "  words_length.append(len(word))\r\n",
        "print(\"Average word length\",sum(words_length)/len(tokenize_words))\r\n",
        "\r\n",
        "#Number of stopwords\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop = stopwords.words('english')\r\n",
        "text = []\r\n",
        "for word in tokenize_words:\r\n",
        "  if word in stop:\r\n",
        "    text.append(word)\r\n",
        "#print(text)\r\n",
        "print(\"number of stopwords\",len(text))\r\n",
        "\r\n",
        "#number of special characters\r\n",
        "char_count=0\r\n",
        "special_chars=['#','@','?','*','!','$','%','^','&','(',')']\r\n",
        "for i in text_data:\r\n",
        "  if i in special_chars:\r\n",
        "    char_count=char_count+1\r\n",
        "print(\"Number of special characters:\",char_count)\r\n",
        "\r\n",
        "#number of numerics\r\n",
        "num_count=0\r\n",
        "for value in text_data:\r\n",
        "  if value.isdigit():\r\n",
        "    num_count+=1\r\n",
        "print(\"number of numeric:\",num_count)\r\n",
        "\r\n",
        "#Number of upper case words\r\n",
        "uppercasewords_count=0\r\n",
        "for word in text_data:\r\n",
        "  if word.isupper():\r\n",
        "    uppercasewords_count+=1\r\n",
        "print(\"number of uppercase words:\",uppercasewords_count)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "NUmber of sentences 173\n",
            "number of words 4368\n",
            "Number of characters 20613\n",
            "Average word length 3.827838827838828\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "number of stopwords 1734\n",
            "Number of special characters: 43\n",
            "number of numeric: 356\n",
            "number of uppercase words: 695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HRXvYSnnOxhG",
        "outputId": "5b866271-edba-4db3-b910-de751bb795db"
      },
      "source": [
        "#1.2\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "#lower casing\r\n",
        "Lower = text_data.lower()\r\n",
        "#print(lower)\r\n",
        "\r\n",
        "#puntuation removal\r\n",
        "Punc_rem = re.sub(r'[^\\w\\s]','',Lower) \r\n",
        "#print(Punc_rem)\r\n",
        "\r\n",
        "#stopwords removal\r\n",
        "text = \" \"\r\n",
        "rem = Punc_rem.split('\\n')\r\n",
        "for i in rem:\r\n",
        "  for j in i.split():\r\n",
        "    if not j in stop:\r\n",
        "      text = text + ' ' + j\r\n",
        "  text = text + '\\n'\r\n",
        "#print(text)\r\n",
        "\r\n",
        "#frequent words removal\r\n",
        "freq_words = pd.Series(text.split()).value_counts()[:20] #freq=pd.Series(data.split()).value_counts()[:10]\r\n",
        "print(\"frequent words:\\n\",freq_words)\r\n",
        "freq_rem = \" \"  \r\n",
        "freq_words = list(freq_words.index)\r\n",
        "for k in text.split('\\n'):\r\n",
        "  for l in k.split():\r\n",
        "    if not l in freq_words:\r\n",
        "      freq_rem = freq_rem + ' ' + l\r\n",
        "  freq_rem = freq_rem + '\\n'\r\n",
        "#print(freq_rem)\r\n",
        "\r\n",
        "#Rare words removal\r\n",
        "rare_words = pd.Series(text.split()).value_counts()[-10:]\r\n",
        "print(\"Rare words:\\n\",rare_words)\r\n",
        "rare_rem = \" \"  \r\n",
        "rare = list(rare_words.index)\r\n",
        "for e in freq_rem.split('\\n'):\r\n",
        "  for f in e.split():\r\n",
        "    if not f in freq_words:\r\n",
        "      rare_rem = rare_rem + ' ' + f\r\n",
        "  rare_rem = rare_rem + '\\n'\r\n",
        "#print(rare_rem)\r\n",
        "\r\n",
        "#spelling correction\r\n",
        "from textblob import TextBlob\r\n",
        "spell_corr = TextBlob(rare_rem).correct()\r\n",
        "#print(spell_corr)\r\n",
        "\r\n",
        "#stemming \r\n",
        "from nltk.stem import PorterStemmer \r\n",
        "ps = PorterStemmer()\r\n",
        "stemming = \" \"                               \r\n",
        "for g in spell_corr.split('\\n'):\r\n",
        "  for h in g.split():\r\n",
        "     stemming = stemming + ' ' + ps.stem(h) #train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "  stemming = stemming + '\\n'\r\n",
        "#print(stemming)\r\n",
        "\r\n",
        "#lemmatization \r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "from textblob import Word\r\n",
        "lemmatization = \" \"\r\n",
        "for a in spell_corr.split('\\n'):\r\n",
        "  for b in a.split():\r\n",
        "     lemmatization = lemmatization + ' ' + Word(b).lemmatize() #train['tweet']=train['sentence'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "  lemmatization = lemmatization + '\\n'\r\n",
        "#print(lemmatization)\r\n",
        "\r\n",
        "#1.3 Saving to a csv file\r\n",
        "import csv\r\n",
        "file = open('final.csv','w') \r\n",
        "file.write(\"output\\n\") \r\n",
        "file.write(lemmatization)\r\n",
        "output_df = pd.read_csv(\"final.csv\",\"\\n\")\r\n",
        "output_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "frequent words:\n",
            " execution     50\n",
            "crop          49\n",
            "lien          25\n",
            "levy          25\n",
            "claimants     22\n",
            "v             22\n",
            "case          21\n",
            "right         21\n",
            "court         20\n",
            "gathered      19\n",
            "growing       16\n",
            "contract      16\n",
            "rep           16\n",
            "ala           15\n",
            "law           15\n",
            "cotton        14\n",
            "2             13\n",
            "possession    13\n",
            "plaintiff     13\n",
            "sale          13\n",
            "dtype: int64\n",
            "Rare words:\n",
            " 7th           1\n",
            "unripe        1\n",
            "assign        1\n",
            "disposes      1\n",
            "contradict    1\n",
            "remedies      1\n",
            "delivered     1\n",
            "rees          1\n",
            "256           1\n",
            "injury        1\n",
            "dtype: int64\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>supreme alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>manner norton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>june term 1843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>filing situation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>negative treatment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>negative treatment result situation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>history</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>history result situation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   output\n",
              "0                                   5 740\n",
              "1                         supreme alabama\n",
              "2                                    adam\n",
              "3                           manner norton\n",
              "4                          june term 1843\n",
              "..                                    ...\n",
              "115                      filing situation\n",
              "116                    negative treatment\n",
              "117   negative treatment result situation\n",
              "118                               history\n",
              "119              history result situation\n",
              "\n",
              "[120 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFJyYVrWUhC3",
        "outputId": "56dcd095-d459-4f6d-cbef-a19fdcb4b501"
      },
      "source": [
        "#1.4\r\n",
        "print(\"Top 10 1-gram\")\r\n",
        "for i in range(10):\r\n",
        " print(TextBlob(output_df['output'][i]).ngrams(1))\r\n",
        "print(\"Top 10 2-grams\")\r\n",
        "for i in range(10):\r\n",
        " print(TextBlob(output_df['output'][i]).ngrams(2))\r\n",
        "print(\"Top 10 3-grams\")\r\n",
        "for i in range(10):\r\n",
        " print(TextBlob(output_df['output'][i]).ngrams(3))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 1-gram\n",
            "[WordList(['5']), WordList(['740'])]\n",
            "[WordList(['supreme']), WordList(['alabama'])]\n",
            "[WordList(['adam'])]\n",
            "[WordList(['manner']), WordList(['norton'])]\n",
            "[WordList(['june']), WordList(['term']), WordList(['1843'])]\n",
            "[WordList(['sycosis'])]\n",
            "[WordList(['writ']), WordList(['error']), WordList(['circuit']), WordList(['sumter'])]\n",
            "[WordList(['west']), WordList(['headnotes'])]\n",
            "[WordList(['1'])]\n",
            "[WordList(['chattel']), WordList(['mortgage'])]\n",
            "Top 10 2-grams\n",
            "[WordList(['5', '740'])]\n",
            "[WordList(['supreme', 'alabama'])]\n",
            "[]\n",
            "[WordList(['manner', 'norton'])]\n",
            "[WordList(['june', 'term']), WordList(['term', '1843'])]\n",
            "[]\n",
            "[WordList(['writ', 'error']), WordList(['error', 'circuit']), WordList(['circuit', 'sumter'])]\n",
            "[WordList(['west', 'headnotes'])]\n",
            "[]\n",
            "[WordList(['chattel', 'mortgage'])]\n",
            "Top 10 3-grams\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[WordList(['june', 'term', '1843'])]\n",
            "[]\n",
            "[WordList(['writ', 'error', 'circuit']), WordList(['error', 'circuit', 'sumter'])]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "e8ZKMXIoVMF3",
        "outputId": "955c5ce7-0080-418c-82dc-815a269423c7"
      },
      "source": [
        "term_frequency = (output_df['output']).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()  \r\n",
        "term_frequency.columns = ['words','term_frequency']\r\n",
        "term_frequency"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>term_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alabama</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>supreme</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>agreed</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>six</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>filing</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>negative</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>history</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>775 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        words  term_frequency\n",
              "0           5             8.0\n",
              "1         740             2.0\n",
              "2                       120.0\n",
              "3     alabama             1.0\n",
              "4     supreme             1.0\n",
              "..        ...             ...\n",
              "770    agreed             1.0\n",
              "771       six             1.0\n",
              "772    filing             2.0\n",
              "773  negative             2.0\n",
              "774   history             2.0\n",
              "\n",
              "[775 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owaY2a98V2u1",
        "outputId": "94209b35-ac53-414b-f3b1-110b2226b6eb"
      },
      "source": [
        "import re\r\n",
        "ip = \"260.08.094.109\"\r\n",
        "string = re.sub('\\.[0]*', '.', ip)\r\n",
        "print(string)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtJUNCEVc0w_",
        "outputId": "5961352d-3fc3-49bf-de2d-7c575967b968"
      },
      "source": [
        "import re\r\n",
        "data = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump\"\r\n",
        "years = re.findall(r'2\\d{3}', data)\r\n",
        "print(years)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}